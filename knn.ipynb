{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8df3dc5-1f1c-4432-9d3e-33519fa4adb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems? \n",
    "#>>K-Nearest Neighbors (KNN) is a supervised learning algorithm used for both classification and regression tasks. It is non-parametric and instance-based, meaning it doesn’t make assumptions about the data distribution and doesn’t explicitly learn a model — instead, it makes predictions based on stored training data.\n",
    "#In Classification:1Each of the k neighbors votes for their class.\n",
    "#                  2The majority class among them becomes the predicted class.\n",
    "#In Regression:1.The algorithm takes the average (or weighted average) of the values of the k nearest neighbors.\n",
    "#              2.The predicted value is this mean of neighbors’ outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fab2621-3fc3-44f1-abb3-0812e10bbe20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. What is the Curse of Dimensionality and how does it affect KNN performance?\n",
    "#>>Curse of Dimensionality refers to the problems that arise when the number of features (dimensions) in the data increases.\n",
    "#KNN relies on distance to find nearest neighbors.\n",
    "#In high dimensions:Distances between points become similar, so it’s hard to identify \"nearest\" neighbors.\n",
    "#                   The algorithm becomes less accurate and computationally expensive.\n",
    "#                   It may require more data to maintain good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b038b45-7e43-42d8-927c-e310c849d6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 What is Principal Component Analysis (PCA)? How is it different from feature selection? \n",
    "#**Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms the original features into a new set of uncorrelated variables called principal components. These components capture the maximum variance in the data using fewer dimensions, helping to simplify datasets while retaining most of their important information.\n",
    "#Unlike feature selection, which chooses a subset of the original features, PCA creates new features (linear combinations of the originals). Thus, PCA transforms the data, while feature selection filters existing features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86ee8fc5-8f65-417e-92cb-04abe11a55b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. What are eigenvalues and eigenvectors in PCA, and why are they important?\n",
    "#>>Principal Component Analysis (PCA), eigenvalues and eigenvectors come from the covariance matrix of the data and are key to identifying the principal components.\n",
    "#Eigenvectors:Represent the directions (axes) along which the data varies the most.\n",
    "#             Each eigenvector corresponds to a principal component.\n",
    "#Eigenvalues:Represent the amount of variance captured by each eigenvector.\n",
    "#            A larger eigenvalue means that direction (component) captures more information (spread) in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6db7fa5d-51b5-4b48-abd1-ec29f58e446f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5How do KNN and PCA complement each other when applied in a single pipeline?\n",
    "#>>K-Nearest Neighbors(KNN) and Principal Component Analysis(PCA) complement each other effectively when used together in a single pipeline. PCA reduces the data’s dimensionality by transforming correlated features into a smaller set of uncorrelated principal components that capture most of the variance.\n",
    "#This simplification helps KNN perform better because it relies on distance calculations, which become more meaningful in lower-dimensional spaces. By removing noise and redundant features, PCA improves KNN’s accuracy, reduces computational cost, and helps overcome the curse of dimensionality, leading to faster and more reliable predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc06394b-0ee3-459f-93bf-1b3f73e0ed03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy without Scaling: 0.722\n",
      "Accuracy with Scaling: 0.944\n"
     ]
    }
   ],
   "source": [
    "#6.Dataset: Use the Wine Dataset from sklearn.datasets.load_wine(). Question 6: Train a KNN Classifier on the Wine dataset with and without feature scaling. Compare model accuracy in both cases. \n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Wine dataset\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 1️⃣ Without Feature Scaling\n",
    "knn_no_scaling = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_no_scaling.fit(X_train, y_train)\n",
    "y_pred_no_scaling = knn_no_scaling.predict(X_test)\n",
    "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
    "# 2️⃣ With Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_scaled.fit(X_train_scaled, y_train)\n",
    "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
    "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "# Results\n",
    "print(\"Accuracy without Scaling:\", round(accuracy_no_scaling, 3))\n",
    "print(\"Accuracy with Scaling:\", round(accuracy_scaled, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f4636d1-bccf-4131-b69a-aba047e53cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio of each Principal Component:\n",
      "PC1: 0.3620\n",
      "PC2: 0.1921\n",
      "PC3: 0.1112\n",
      "PC4: 0.0707\n",
      "PC5: 0.0656\n",
      "PC6: 0.0494\n",
      "PC7: 0.0424\n",
      "PC8: 0.0268\n",
      "PC9: 0.0222\n",
      "PC10: 0.0193\n",
      "PC11: 0.0174\n",
      "PC12: 0.0130\n",
      "PC13: 0.0080\n",
      "\n",
      "Cumulative Explained Variance: [0.36198848 0.55406338 0.66529969 0.73598999 0.80162293 0.85098116\n",
      " 0.89336795 0.92017544 0.94239698 0.96169717 0.97906553 0.99204785\n",
      " 1.        ]\n"
     ]
    }
   ],
   "source": [
    "#7Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component. \n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Load the Wine dataset\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "\n",
    "# Step 1: Standardize the data (important before PCA)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 2: Apply PCA\n",
    "pca = PCA()\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "# Step 3: Print the explained variance ratio\n",
    "print(\"Explained Variance Ratio of each Principal Component:\")\n",
    "for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
    "    print(f\"PC{i+1}: {ratio:.4f}\")\n",
    "\n",
    "# Optional: print cumulative variance\n",
    "print(\"\\nCumulative Explained Variance:\", np.cumsum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0c3d682-fd7b-4d52-b376-23a2ba9c66e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Original (Scaled) Dataset: 0.944\n",
      "Accuracy on PCA-Transformed Dataset (2 Components): 1.0\n"
     ]
    }
   ],
   "source": [
    "#8 Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset.\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Wine dataset\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 1: Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 2: Train KNN on the original (scaled) dataset\n",
    "knn_original = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_original.fit(X_train_scaled, y_train)\n",
    "y_pred_original = knn_original.predict(X_test_scaled)\n",
    "accuracy_original = accuracy_score(y_test, y_pred_original)\n",
    "\n",
    "# Step 3: Apply PCA (retain top 2 components)\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Step 4: Train KNN on the PCA-transformed dataset\n",
    "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_pca.fit(X_train_pca, y_train)\n",
    "y_pred_pca = knn_pca.predict(X_test_pca)\n",
    "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
    "\n",
    "# Step 5: Compare results\n",
    "print(\"Accuracy on Original (Scaled) Dataset:\", round(accuracy_original, 3))\n",
    "print(\"Accuracy on PCA-Transformed Dataset (2 Components):\", round(accuracy_pca, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d52f8a78-f3e2-4172-b115-36795fac0dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Euclidean Distance: 0.944\n",
      "Accuracy with Manhattan Distance: 0.944\n"
     ]
    }
   ],
   "source": [
    "#9Train a KNN Classifier with different distance metrics (euclidean, manhattan) on the scaled Wine dataset and compare the results. \n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Wine dataset\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 1: Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 2: Train KNN with Euclidean distance (default)\n",
    "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "knn_euclidean.fit(X_train_scaled, y_train)\n",
    "y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n",
    "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
    "\n",
    "# Step 3: Train KNN with Manhattan distance\n",
    "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
    "knn_manhattan.fit(X_train_scaled, y_train)\n",
    "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
    "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
    "\n",
    "# Step 4: Compare results\n",
    "print(\"Accuracy with Euclidean Distance:\", round(accuracy_euclidean, 3))\n",
    "print(\"Accuracy with Manhattan Distance:\", round(accuracy_manhattan, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ebc423-b549-498a-b16f-b0596ee0ab02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
